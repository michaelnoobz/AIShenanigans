<h1>Local LLM Deployment with Web UI â€” Gemma 12B on ASUS Zenbook</h1>

<p>This project showcases the successful local deployment of the <strong>Gemma 12B</strong> large language model using a <strong>Web UI</strong> interface on an <strong>ASUS Zenbook (2024)</strong>.</p>

<h2>ðŸ’» Specs</h2>
<ul>
  <li><strong>Laptop:</strong> ASUS Zenbook (2024)</li>
  <li><strong>CPU:</strong> Intel Core Ultra i7</li>
  <li><strong>RAM:</strong> 32GB</li>
  <li><strong>Storage:</strong> 1TB SSD</li>
  <li><strong>OS:</strong> Windows 11</li>
  <li><strong>Interface:</strong> Web UI (local deployment)</li>
</ul>

<h2>ðŸš€ What I Did</h2>
<ul>
  <li>Deployed <strong>Gemma 12B</strong> (12 billion parameter LLM) locally via Web UI on CPU.</li>
  <li>No GPU used â€” this was done entirely with CPU resources.</li>
  <li>Managed storage limit
