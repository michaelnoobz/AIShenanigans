<h1>Local LLM Deployment with Web UI â€” Gemma 12B on ASUS Zenbook</h1>

<p>This project showcases the successful local deployment of the <strong>Gemma 12B</strong> large language model using a <strong>Web UI</strong> interface on an <strong>ASUS Zenbook (2024)</strong>.</p>

<h2>ğŸ’» Specs</h2>
<ul>
  <li><strong>Laptop:</strong> ASUS Zenbook (2024)</li>
  <li><strong>CPU:</strong> Intel Core Ultra i7</li>
  <li><strong>RAM:</strong> 32GB</li>
  <li><strong>Storage:</strong> 1TB SSD</li>
  <li><strong>OS:</strong> Windows 11</li>
  <li><strong>Interface:</strong> Web UI (local deployment)</li>
</ul>

<h2>ğŸš€ What I Did</h2>
<ul>
  <li>Deployed <strong>Gemma 12B</strong> (12 billion parameter LLM) locally via Web UI on CPU.</li>
  <li>No GPU used â€” this was done entirely with CPU resources.</li>
  <li>Managed storage limitations and tuned performance for better results.</li>
  <li>Experimented with prompt engineering and real-time chat through the web interface.</li>
  <li>Tested feasibility of running LLMs locally for private AI assistant use cases.</li>
</ul>

<h2>ğŸ§  Why This Matters</h2>
<p>Running LLMs like Gemma locally â€” with just a modern laptop and no GPU â€” proves that AI experimentation is accessible. This approach enables:</p>
<ul>
  <li>Self-hosted, privacy-friendly AI agents</li>
  <li>Offline experimentation without relying on cloud services</li>
  <li>Deeper understanding of model loading, resources, and performance tuning</li>
</ul>

<h2>ğŸ–¼ï¸ Screenshots</h2>

<h3>Web UI Running Locally</h3>
<img src="web-ui-screenshot.png" alt="Web UI Screenshot" width="600"/>

<h3>Gemma 12B Loaded and Running</h3>
<img src="gemma-12b-screenshot.png" alt="Gemma 12B Screenshot" width="600"/>

<hr/>

<h2>ğŸ”— Connect With Me</h2>
<ul>
  <li><a href="https://www.linkedin.com/in/michael-lannen-053588167" target="_blank">LinkedIn â€“ Michael Lannen</a></li>
  <li><a href="https://github.com/michaelnoobz" target="_blank">GitHub â€“ michaelnoobz</a></li>
</ul>
